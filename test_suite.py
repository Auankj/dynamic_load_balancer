"""
Comprehensive Test Suite for Dynamic Load Balancing Simulator

This module provides extensive unit tests, integration tests, and scenario
tests for all components of the load balancing simulation system.

Test Categories:
1. Unit Tests: Test individual classes and methods in isolation
2. Integration Tests: Test component interactions
3. Scenario Tests: Test realistic use cases with various configurations
4. Edge Case Tests: Test boundary conditions and error handling
5. Performance Tests: Basic performance benchmarks

Testing Framework: unittest (Python standard library)

Author: Student
Date: December 2024
"""

import unittest
import sys
import os
import time
import logging
import json
import random
import statistics
from unittest.mock import MagicMock, patch
from dataclasses import dataclass
from typing import List, Dict, Any

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import project modules
from config import (
    ProcessState,
    ProcessPriority,
    LoadBalancingAlgorithm,
    SimulationConfig,
    GUIConfig,
    DEFAULT_SIMULATION_CONFIG,
    DEFAULT_GUI_CONFIG
)
from process import Process, ProcessGenerator
from processor import Processor, ProcessorManager, ProcessorStatistics
from load_balancer import (
    LoadBalancer,
    LoadBalancerFactory,
    RoundRobinBalancer,
    LeastLoadedBalancer,
    ThresholdBasedBalancer,
    MigrationRecord
)
from simulation import SimulationEngine, SimulationState, SimulationResult
from metrics import (
    ProcessMetrics,
    ProcessorMetrics,
    SystemMetrics,
    MetricsCalculator,
    MetricsComparator
)
from utils import SimulationLogger, DataExporter

# Disable logging during tests unless debugging
logging.disable(logging.CRITICAL)


# =============================================================================
# TEST CONFIGURATION
# =============================================================================

class TestConfig(unittest.TestCase):
    """Test configuration module and data classes."""
    
    def test_process_state_enum_values(self):
        """Test that all process states are defined."""
        states = [ProcessState.NEW, ProcessState.READY, ProcessState.RUNNING,
                  ProcessState.WAITING, ProcessState.COMPLETED, ProcessState.MIGRATING]
        self.assertEqual(len(states), 6)
        
    def test_process_priority_enum_values(self):
        """Test priority ordering."""
        self.assertLess(ProcessPriority.HIGH.value, ProcessPriority.MEDIUM.value)
        self.assertLess(ProcessPriority.MEDIUM.value, ProcessPriority.LOW.value)
        
    def test_load_balancing_algorithm_enum(self):
        """Test all load balancing algorithms are defined."""
        self.assertEqual(LoadBalancingAlgorithm.ROUND_ROBIN.value, "Round Robin")
        self.assertEqual(LoadBalancingAlgorithm.LEAST_LOADED.value, "Least Loaded First")
        self.assertEqual(LoadBalancingAlgorithm.THRESHOLD_BASED.value, "Threshold Based")
        self.assertEqual(LoadBalancingAlgorithm.Q_LEARNING.value, "AI (Q-Learning)")
        
    def test_simulation_config_defaults(self):
        """Test default configuration values."""
        config = SimulationConfig()
        self.assertEqual(config.num_processors, 4)
        self.assertGreaterEqual(config.num_processes, 1)
        self.assertGreater(config.time_quantum, 0)
        
    def test_simulation_config_validation(self):
        """Test configuration validation."""
        config = SimulationConfig()
        self.assertGreaterEqual(config.min_processors, 1)
        self.assertLessEqual(config.max_processors, 16)
        
    def test_gui_config_defaults(self):
        """Test GUI configuration defaults."""
        config = GUIConfig()
        self.assertIsNotNone(config.window_title)
        self.assertGreater(config.window_width, 0)
        self.assertGreater(config.window_height, 0)


# =============================================================================
# TEST PROCESS MODULE
# =============================================================================

class TestProcess(unittest.TestCase):
    """Test Process class and related functionality."""
    
    def test_process_creation(self):
        """Test basic process creation."""
        process = Process(pid=1, arrival_time=0, burst_time=10)
        self.assertEqual(process.pid, 1)
        self.assertEqual(process.arrival_time, 0)
        self.assertEqual(process.burst_time, 10)
        self.assertEqual(process.remaining_time, 10)
        self.assertEqual(process.state, ProcessState.NEW)
        
    def test_process_remaining_time_init(self):
        """Test remaining time initializes to burst time."""
        process = Process(pid=1, burst_time=15)
        self.assertEqual(process.remaining_time, process.burst_time)
        
    def test_process_priority_levels(self):
        """Test different priority levels."""
        high = Process(pid=1, priority=ProcessPriority.HIGH)
        medium = Process(pid=2, priority=ProcessPriority.MEDIUM)
        low = Process(pid=3, priority=ProcessPriority.LOW)
        
        self.assertEqual(high.priority, ProcessPriority.HIGH)
        self.assertEqual(medium.priority, ProcessPriority.MEDIUM)
        self.assertEqual(low.priority, ProcessPriority.LOW)
        
    def test_process_state_changes(self):
        """Test process state transitions."""
        process = Process(pid=1)
        
        # NEW -> READY
        process.state = ProcessState.READY
        self.assertEqual(process.state, ProcessState.READY)
        
        # READY -> RUNNING
        process.state = ProcessState.RUNNING
        self.assertEqual(process.state, ProcessState.RUNNING)
        
        # RUNNING -> COMPLETED
        process.state = ProcessState.COMPLETED
        self.assertEqual(process.state, ProcessState.COMPLETED)
        
    def test_process_execution_history(self):
        """Test execution history tracking."""
        process = Process(pid=1)
        self.assertIsInstance(process.execution_history, list)
        self.assertEqual(len(process.execution_history), 0)
        
    def test_process_migration_count_init(self):
        """Test migration count initialization."""
        process = Process(pid=1)
        self.assertEqual(process.migration_count, 0)
        
    def test_process_is_completed_property(self):
        """Test is_completed property if exists."""
        process = Process(pid=1, burst_time=10)
        process.remaining_time = 0
        if hasattr(process, 'is_completed'):
            self.assertTrue(process.is_completed)
            
    def test_process_to_dict(self):
        """Test process serialization."""
        process = Process(pid=1, arrival_time=5, burst_time=10)
        if hasattr(process, 'to_dict'):
            data = process.to_dict()
            self.assertEqual(data['pid'], 1)
            self.assertEqual(data['arrival_time'], 5)


class TestProcessGenerator(unittest.TestCase):
    """Test ProcessGenerator functionality."""
    
    def test_generate_processes(self):
        """Test process generation."""
        generator = ProcessGenerator()
        processes = generator.generate_processes(10)
        
        self.assertEqual(len(processes), 10)
        for i, proc in enumerate(processes):
            self.assertIsInstance(proc, Process)
            self.assertGreater(proc.burst_time, 0)
            
    def test_unique_pids(self):
        """Test that generated processes have unique PIDs."""
        generator = ProcessGenerator()
        processes = generator.generate_processes(20)
        
        pids = [p.pid for p in processes]
        self.assertEqual(len(pids), len(set(pids)))  # All unique
        
    def test_generate_with_reset(self):
        """Test process generation with reset."""
        config = SimulationConfig()
        gen1 = ProcessGenerator(config)
        
        procs1 = gen1.generate_processes(5)
        first_last_pid = procs1[-1].pid
        
        # After generating more, PID should continue
        procs2 = gen1.generate_processes(5)
        
        # PIDs should continue incrementing
        self.assertGreater(procs2[0].pid, first_last_pid)
            
    def test_burst_time_bounds(self):
        """Test burst times are within configured bounds."""
        config = SimulationConfig()
        generator = ProcessGenerator(config)
        processes = generator.generate_processes(50)
        
        for proc in processes:
            self.assertGreaterEqual(proc.burst_time, config.min_burst_time)
            self.assertLessEqual(proc.burst_time, config.max_burst_time)
            
    def test_arrival_time_generation(self):
        """Test arrival times are properly distributed."""
        config = SimulationConfig()
        generator = ProcessGenerator(config)
        processes = generator.generate_processes(10)
        
        for proc in processes:
            self.assertGreaterEqual(proc.arrival_time, 0)


# =============================================================================
# TEST PROCESSOR MODULE
# =============================================================================

class TestProcessor(unittest.TestCase):
    """Test Processor class."""
    
    def test_processor_creation(self):
        """Test processor initialization."""
        processor = Processor(processor_id=0)
        self.assertEqual(processor.processor_id, 0)
        self.assertEqual(processor.get_queue_size(), 0)
        
    def test_processor_add_process(self):
        """Test adding process to processor."""
        processor = Processor(processor_id=0)
        process = Process(pid=1, burst_time=10)
        
        result = processor.add_process(process)
        self.assertTrue(result)
        self.assertGreaterEqual(processor.get_queue_size(), 1)
        
    def test_processor_queue_size(self):
        """Test queue size tracking."""
        processor = Processor(processor_id=0)
        
        for i in range(5):
            processor.add_process(Process(pid=i+1, burst_time=5))
            
        self.assertGreaterEqual(processor.get_queue_size(), 1)
        
    def test_processor_get_load(self):
        """Test load calculation."""
        processor = Processor(processor_id=0)
        
        # Empty processor should have low load
        initial_load = processor.get_load()
        self.assertGreaterEqual(initial_load, 0)
        
        # Add processes
        for i in range(3):
            processor.add_process(Process(pid=i+1, burst_time=10))
            
        # Load should increase
        loaded = processor.get_load()
        self.assertGreaterEqual(loaded, initial_load)
        
    def test_processor_execute_step(self):
        """Test process execution."""
        processor = Processor(processor_id=0)
        process = Process(pid=1, burst_time=10)
        processor.add_process(process)
        
        # Execute should not raise
        if hasattr(processor, 'execute_time_slice'):
            processor.execute_time_slice(current_time=1)
        elif hasattr(processor, 'execute_step'):
            processor.execute_step(current_time=1)
            
    def test_processor_is_idle(self):
        """Test idle state detection."""
        processor = Processor(processor_id=0)
        
        # Should be idle when empty
        if hasattr(processor, 'is_idle'):
            self.assertTrue(processor.is_idle())
            
        if hasattr(processor, 'has_processes'):
            self.assertFalse(processor.has_processes())
            
        processor.add_process(Process(pid=1, burst_time=5))
        
        if hasattr(processor, 'has_processes'):
            self.assertTrue(processor.has_processes())
            
    def test_processor_statistics(self):
        """Test statistics tracking."""
        processor = Processor(processor_id=0)
        
        if hasattr(processor, 'statistics'):
            self.assertIsInstance(processor.statistics, ProcessorStatistics)
            
    def test_processor_get_current_process(self):
        """Test getting current executing process."""
        processor = Processor(processor_id=0)
        process = Process(pid=1, burst_time=10)
        processor.add_process(process)
        
        # current_process may be None if not executing yet
        self.assertTrue(hasattr(processor, 'current_process'))


class TestProcessorManager(unittest.TestCase):
    """Test ProcessorManager class."""
    
    def test_manager_creation(self):
        """Test processor manager initialization."""
        manager = ProcessorManager(num_processors=4)
        self.assertEqual(len(manager.processors), 4)
        
    def test_manager_get_processor(self):
        """Test getting specific processor."""
        manager = ProcessorManager(num_processors=4)
        
        proc0 = manager.get_processor(0)
        self.assertIsNotNone(proc0)
        self.assertEqual(proc0.processor_id, 0)
        
    def test_manager_get_all_loads(self):
        """Test getting all processor loads."""
        manager = ProcessorManager(num_processors=4)
        
        if hasattr(manager, 'get_all_loads'):
            loads = manager.get_all_loads()
            self.assertEqual(len(loads), 4)
            
    def test_manager_get_least_loaded(self):
        """Test finding least loaded processor."""
        manager = ProcessorManager(num_processors=4)
        
        # Add different loads
        manager.processors[0].add_process(Process(pid=1, burst_time=20))
        manager.processors[0].add_process(Process(pid=2, burst_time=20))
        manager.processors[1].add_process(Process(pid=3, burst_time=5))
        
        if hasattr(manager, 'get_least_loaded_processor'):
            least = manager.get_least_loaded_processor()
            self.assertIsNotNone(least)
            
    def test_manager_total_processes(self):
        """Test counting total processes."""
        manager = ProcessorManager(num_processors=4)
        
        manager.processors[0].add_process(Process(pid=1, burst_time=5))
        manager.processors[1].add_process(Process(pid=2, burst_time=5))
        manager.processors[1].add_process(Process(pid=3, burst_time=5))
        
        if hasattr(manager, 'get_total_processes'):
            total = manager.get_total_processes()
            self.assertEqual(total, 3)


# =============================================================================
# TEST LOAD BALANCER MODULE
# =============================================================================

class TestRoundRobinBalancer(unittest.TestCase):
    """Test Round Robin load balancing algorithm."""
    
    def test_round_robin_creation(self):
        """Test Round Robin balancer creation."""
        balancer = RoundRobinBalancer()
        self.assertEqual(balancer.algorithm_type, LoadBalancingAlgorithm.ROUND_ROBIN)
        
    def test_round_robin_cyclic_assignment(self):
        """Test cyclic assignment pattern."""
        config = SimulationConfig(num_processors=4)
        balancer = RoundRobinBalancer(config)
        manager = ProcessorManager(num_processors=4)
        
        assigned_processors = []
        for i in range(8):
            process = Process(pid=i, burst_time=5)
            processor = balancer.assign_process(process, manager.processors)
            if processor:
                assigned_processors.append(processor.processor_id)
                
        # Should cycle through processors
        # Expect pattern like 0, 1, 2, 3, 0, 1, 2, 3
        for i in range(min(4, len(assigned_processors))):
            self.assertEqual(assigned_processors[i] % 4, i)
            
    def test_round_robin_handles_empty_processors(self):
        """Test handling when processors list is empty."""
        balancer = RoundRobinBalancer()
        process = Process(pid=1, burst_time=5)
        
        result = balancer.assign_process(process, [])
        self.assertIsNone(result)


class TestLeastLoadedBalancer(unittest.TestCase):
    """Test Least Loaded First load balancing algorithm."""
    
    def test_least_loaded_creation(self):
        """Test Least Loaded balancer creation."""
        balancer = LeastLoadedBalancer()
        self.assertEqual(balancer.algorithm_type, LoadBalancingAlgorithm.LEAST_LOADED)
        
    def test_least_loaded_selects_minimum(self):
        """Test that it selects the least loaded processor."""
        config = SimulationConfig(num_processors=4)
        balancer = LeastLoadedBalancer(config)
        manager = ProcessorManager(num_processors=4)
        
        # Load processors differently
        manager.processors[0].add_process(Process(pid=100, burst_time=50))
        manager.processors[0].add_process(Process(pid=101, burst_time=50))
        manager.processors[1].add_process(Process(pid=102, burst_time=30))
        manager.processors[2].add_process(Process(pid=103, burst_time=10))
        # Processor 3 is empty - should be selected
        
        process = Process(pid=1, burst_time=5)
        selected = balancer.assign_process(process, manager.processors)
        
        # Should select processor 3 (least loaded)
        self.assertIsNotNone(selected)
        
    def test_least_loaded_load_balancing(self):
        """Test load balancing effectiveness."""
        config = SimulationConfig(num_processors=4)
        balancer = LeastLoadedBalancer(config)
        manager = ProcessorManager(num_processors=4)
        
        # Assign many processes
        for i in range(20):
            process = Process(pid=i, burst_time=random.randint(5, 15))
            balancer.assign_process(process, manager.processors)
            
        # Get loads
        loads = [p.get_load() for p in manager.processors]
        
        # Check load variance is reasonable (should be more balanced)
        if any(loads):
            load_variance = statistics.variance(loads) if len(loads) > 1 else 0
            # Just verify it runs - variance check is informational
            self.assertGreaterEqual(load_variance, 0)


class TestThresholdBasedBalancer(unittest.TestCase):
    """Test Threshold Based load balancing algorithm."""
    
    def test_threshold_based_creation(self):
        """Test Threshold Based balancer creation."""
        balancer = ThresholdBasedBalancer()
        self.assertEqual(balancer.algorithm_type, LoadBalancingAlgorithm.THRESHOLD_BASED)
        
    def test_threshold_based_migration_check(self):
        """Test migration decision based on threshold."""
        config = SimulationConfig(num_processors=4, load_threshold=0.3)
        balancer = ThresholdBasedBalancer(config)
        manager = ProcessorManager(num_processors=4)
        
        # Create imbalanced load
        for _ in range(5):
            manager.processors[0].add_process(
                Process(pid=random.randint(100, 200), burst_time=20)
            )
            
        # Check for migrations
        if hasattr(balancer, 'check_for_migration'):
            migrations = balancer.check_for_migration(manager.processors, current_time=0)
            # May or may not trigger migration depending on threshold
            self.assertIsInstance(migrations, (list, type(None)))


class TestLoadBalancerFactory(unittest.TestCase):
    """Test LoadBalancerFactory."""
    
    def test_factory_creates_round_robin(self):
        """Test factory creates Round Robin balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.ROUND_ROBIN)
        self.assertIsInstance(balancer, RoundRobinBalancer)
        
    def test_factory_creates_least_loaded(self):
        """Test factory creates Least Loaded balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.LEAST_LOADED)
        self.assertIsInstance(balancer, LeastLoadedBalancer)
        
    def test_factory_creates_threshold_based(self):
        """Test factory creates Threshold Based balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.THRESHOLD_BASED)
        self.assertIsInstance(balancer, ThresholdBasedBalancer)
        
    def test_factory_creates_qlearning(self):
        """Test factory creates Q-Learning balancer."""
        from ai_balancer import QLearningBalancer
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.Q_LEARNING, num_processors=4)
        self.assertIsInstance(balancer, QLearningBalancer)
        
    def test_factory_with_config(self):
        """Test factory passes configuration."""
        config = SimulationConfig(num_processors=8)
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.ROUND_ROBIN, config)
        self.assertEqual(balancer.config.num_processors, 8)
    
    def test_factory_creates_fcfs(self):
        """Test factory creates FCFS balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.FCFS)
        self.assertEqual(balancer.name, "FCFS (First Come First Served)")
    
    def test_factory_creates_sjf(self):
        """Test factory creates SJF balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.SJF)
        self.assertEqual(balancer.name, "SJF (Shortest Job First)")
    
    def test_factory_creates_priority(self):
        """Test factory creates Priority balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.PRIORITY)
        self.assertIn("Priority", balancer.name)
    
    def test_factory_creates_priority_preemptive(self):
        """Test factory creates Preemptive Priority balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.PRIORITY_PREEMPTIVE)
        self.assertIn("Preemptive", balancer.name)
    
    def test_factory_creates_mlfq(self):
        """Test factory creates MLFQ balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.MLFQ)
        self.assertIn("MLFQ", balancer.name)
    
    def test_factory_creates_edf(self):
        """Test factory creates EDF balancer."""
        balancer = LoadBalancerFactory.create(LoadBalancingAlgorithm.EDF)
        self.assertEqual(balancer.name, "EDF (Earliest Deadline First)")
    
    def test_factory_all_algorithms_count(self):
        """Test that all 13 algorithms are available."""
        all_algos = LoadBalancerFactory.get_all_algorithms()
        self.assertEqual(len(all_algos), 13)
    
    def test_factory_load_balancing_vs_scheduling(self):
        """Test separation of load balancing and scheduling algorithms."""
        lb_algos = LoadBalancerFactory.get_load_balancing_algorithms()
        sched_algos = LoadBalancerFactory.get_scheduling_algorithms()
        self.assertEqual(len(lb_algos), 5)  # RR, LL, Threshold, Q-Learn, DQN
        self.assertEqual(len(sched_algos), 8)  # FCFS, SJF, SRTF, Priority x2, MLQ, MLFQ, EDF


# =============================================================================
# TEST SCHEDULING ALGORITHMS
# =============================================================================

class TestSchedulingAlgorithms(unittest.TestCase):
    """Test advanced CPU scheduling algorithms."""
    
    def setUp(self):
        """Set up test fixtures."""
        from scheduling_algorithms import (
            FCFSScheduler, SJFScheduler, SRTFScheduler,
            PriorityScheduler, MultilevelQueueScheduler,
            MLFQScheduler, EDFScheduler, ExtendedProcess
        )
        self.FCFSScheduler = FCFSScheduler
        self.SJFScheduler = SJFScheduler
        self.SRTFScheduler = SRTFScheduler
        self.PriorityScheduler = PriorityScheduler
        self.MultilevelQueueScheduler = MultilevelQueueScheduler
        self.MLFQScheduler = MLFQScheduler
        self.EDFScheduler = EDFScheduler
        self.ExtendedProcess = ExtendedProcess
    
    def _create_test_process(self, pid, burst_time, arrival_time=0, priority=ProcessPriority.MEDIUM):
        """Helper to create test process."""
        return Process(pid=pid, burst_time=burst_time, arrival_time=arrival_time, priority=priority)
    
    def test_fcfs_is_non_preemptive(self):
        """Test that FCFS is non-preemptive."""
        scheduler = self.FCFSScheduler()
        self.assertFalse(scheduler.is_preemptive)
    
    def test_fcfs_fifo_order(self):
        """Test FCFS processes in arrival order."""
        scheduler = self.FCFSScheduler()
        p1 = self._create_test_process(1, burst_time=5, arrival_time=0)
        p2 = self._create_test_process(2, burst_time=3, arrival_time=1)
        p3 = self._create_test_process(3, burst_time=2, arrival_time=2)
        
        scheduler.add_process(p1)
        scheduler.add_process(p2)
        scheduler.add_process(p3)
        
        # First selected should be p1 (arrived first)
        selected = scheduler.select_next_process()
        self.assertEqual(selected.pid, 1)
    
    def test_sjf_selects_shortest(self):
        """Test SJF selects shortest job."""
        scheduler = self.SJFScheduler()
        p1 = self._create_test_process(1, burst_time=10)
        p2 = self._create_test_process(2, burst_time=3)  # Shortest
        p3 = self._create_test_process(3, burst_time=7)
        
        scheduler.add_process(p1)
        scheduler.add_process(p2)
        scheduler.add_process(p3)
        
        selected = scheduler.select_next_process()
        self.assertEqual(selected.pid, 2)  # Shortest job
    
    def test_sjf_is_non_preemptive(self):
        """Test that SJF is non-preemptive."""
        scheduler = self.SJFScheduler()
        self.assertFalse(scheduler.is_preemptive)
    
    def test_srtf_is_preemptive(self):
        """Test that SRTF is preemptive."""
        scheduler = self.SRTFScheduler()
        self.assertTrue(scheduler.is_preemptive)
    
    def test_srtf_selects_shortest_remaining(self):
        """Test SRTF selects process with shortest remaining time."""
        scheduler = self.SRTFScheduler()
        p1 = self._create_test_process(1, burst_time=10)
        p2 = self._create_test_process(2, burst_time=2)  # Shortest remaining
        p3 = self._create_test_process(3, burst_time=5)
        
        scheduler.add_process(p1)
        scheduler.add_process(p2)
        scheduler.add_process(p3)
        
        selected = scheduler.select_next_process()
        self.assertEqual(selected.pid, 2)
    
    def test_priority_scheduler_modes(self):
        """Test priority scheduler preemptive/non-preemptive modes."""
        non_preemptive = self.PriorityScheduler(preemptive=False)
        preemptive = self.PriorityScheduler(preemptive=True)
        
        self.assertFalse(non_preemptive.is_preemptive)
        self.assertTrue(preemptive.is_preemptive)
    
    def test_priority_selects_highest(self):
        """Test priority scheduler selects highest priority (lowest number)."""
        scheduler = self.PriorityScheduler()
        p1 = self._create_test_process(1, burst_time=5, priority=ProcessPriority.LOW)
        p2 = self._create_test_process(2, burst_time=5, priority=ProcessPriority.HIGH)
        p3 = self._create_test_process(3, burst_time=5, priority=ProcessPriority.MEDIUM)
        
        scheduler.add_process(p1)
        scheduler.add_process(p2)
        scheduler.add_process(p3)
        
        selected = scheduler.select_next_process()
        self.assertEqual(selected.pid, 2)  # HIGH priority
    
    def test_multilevel_queue_structure(self):
        """Test multilevel queue has separate queues."""
        from scheduling_algorithms import QueueLevel
        scheduler = self.MultilevelQueueScheduler()
        
        # Should have queues for each level
        self.assertEqual(len(scheduler.queues), 4)  # SYSTEM, INTERACTIVE, BATCH, IDLE
        for level in QueueLevel:
            self.assertIn(level, scheduler.queues)
    
    def test_mlfq_starts_at_top(self):
        """Test MLFQ adds new processes to top queue."""
        scheduler = self.MLFQScheduler(num_queues=4)
        p = self._create_test_process(1, burst_time=10)
        
        scheduler.add_process(p)
        
        # New process should be in top queue (index 0)
        self.assertEqual(len(scheduler.queues[0]), 1)
    
    def test_mlfq_parameters(self):
        """Test MLFQ configurable parameters."""
        scheduler = self.MLFQScheduler(num_queues=3, base_quantum=4, boost_interval=100)
        
        self.assertEqual(scheduler.num_queues, 3)
        self.assertEqual(scheduler.base_quantum, 4)
        self.assertEqual(scheduler.boost_interval, 100)
        # Quantums should double per level: 4, 8, 16
        self.assertEqual(scheduler.quantums, [4, 8, 16])
    
    def test_edf_is_preemptive(self):
        """Test EDF is preemptive."""
        scheduler = self.EDFScheduler()
        self.assertTrue(scheduler.is_preemptive)
    
    def test_edf_selects_earliest_deadline(self):
        """Test EDF selects process with earliest deadline."""
        scheduler = self.EDFScheduler()
        p1 = self._create_test_process(1, burst_time=5)
        p2 = self._create_test_process(2, burst_time=5)
        p3 = self._create_test_process(3, burst_time=5)
        
        scheduler.add_process(p1, deadline=30)
        scheduler.add_process(p2, deadline=10)  # Earliest
        scheduler.add_process(p3, deadline=20)
        
        selected = scheduler.select_next_process()
        self.assertEqual(selected.pid, 2)  # Earliest deadline
    
    def test_scheduler_statistics(self):
        """Test scheduler tracks statistics."""
        scheduler = self.FCFSScheduler()
        p = self._create_test_process(1, burst_time=5)
        scheduler.add_process(p)
        
        stats = scheduler.get_statistics()
        self.assertIn('total_processes', stats)
        self.assertIn('completed_processes', stats)
        self.assertIn('context_switches', stats)
        self.assertEqual(stats['total_processes'], 1)
    
    def test_scheduler_reset(self):
        """Test scheduler can be reset."""
        scheduler = self.SJFScheduler()
        p = self._create_test_process(1, burst_time=5)
        scheduler.add_process(p)
        
        scheduler.reset()
        
        self.assertEqual(len(scheduler.ready_queue), 0)
        self.assertIsNone(scheduler.running_process)


class TestSchedulerFactory(unittest.TestCase):
    """Test SchedulerFactory."""
    
    def test_create_all_schedulers(self):
        """Test factory can create all scheduler types."""
        from scheduling_algorithms import SchedulerFactory, SchedulingAlgorithm
        
        schedulers = [
            SchedulingAlgorithm.FCFS,
            SchedulingAlgorithm.SJF,
            SchedulingAlgorithm.SRTF,
            SchedulingAlgorithm.PRIORITY,
            SchedulingAlgorithm.PRIORITY_PREEMPTIVE,
            SchedulingAlgorithm.MULTILEVEL_QUEUE,
            SchedulingAlgorithm.MLFQ,
            SchedulingAlgorithm.EDF,
        ]
        
        for algo in schedulers:
            scheduler = SchedulerFactory.create_scheduler(algo)
            self.assertIsNotNone(scheduler)
    
    def test_algorithm_info(self):
        """Test algorithm info is comprehensive."""
        from scheduling_algorithms import SchedulerFactory
        
        info = SchedulerFactory.get_algorithm_info()
        self.assertGreater(len(info), 0)
        
        for name, details in info.items():
            self.assertIn('preemptive', details)
            self.assertIn('pros', details)
            self.assertIn('cons', details)


# =============================================================================
# TEST SIMULATION ENGINE
# =============================================================================

class TestSimulationEngine(unittest.TestCase):
    """Test SimulationEngine class."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.config = SimulationConfig(num_processors=4, num_processes=10)
        self.engine = SimulationEngine(self.config)
        
    def test_engine_creation(self):
        """Test engine initialization."""
        self.assertIsNotNone(self.engine)
        self.assertEqual(self.engine.state, SimulationState.IDLE)
        
    def test_engine_initialize(self):
        """Test engine initialization."""
        result = self.engine.initialize()
        self.assertTrue(result)
        self.assertIsNotNone(self.engine.processor_manager)
        self.assertIsNotNone(self.engine.load_balancer)
        
    def test_engine_start_stop(self):
        """Test start and stop functionality."""
        self.engine.initialize()
        
        # Run a few steps
        self.engine.step()
        self.engine.step()
        
        self.engine.stop()
        self.assertIn(self.engine.state, 
                     [SimulationState.STOPPED, SimulationState.COMPLETED])
        
    def test_engine_step(self):
        """Test single simulation step."""
        self.engine.initialize()
        initial_time = self.engine.current_time
        
        self.engine.step()
        
        # Time should advance
        self.assertGreater(self.engine.current_time, initial_time)
        
    def test_engine_reset(self):
        """Test engine reset via reinitialize."""
        self.engine.initialize()
        self.engine.step()
        self.engine.step()
        
        # Re-initialize resets the engine
        self.engine.initialize()
        
        self.assertEqual(self.engine.state, SimulationState.IDLE)
        self.assertEqual(self.engine.current_time, 0)
        
    def test_engine_run_to_completion(self):
        """Test running simulation to completion."""
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        self.assertIsInstance(result, SimulationResult)
        self.assertIn(engine.state, 
                     [SimulationState.COMPLETED, SimulationState.STOPPED])
        
    def test_engine_change_algorithm(self):
        """Test changing load balancing algorithm."""
        # Test Least Loaded
        self.engine.initialize(algorithm=LoadBalancingAlgorithm.LEAST_LOADED)
        self.assertEqual(
            self.engine.load_balancer.algorithm_type,
            LoadBalancingAlgorithm.LEAST_LOADED
        )
        
        # Create new engine for different algorithm test
        engine2 = SimulationEngine(self.config)
        engine2.initialize(algorithm=LoadBalancingAlgorithm.THRESHOLD_BASED)
        self.assertEqual(
            engine2.load_balancer.algorithm_type,
            LoadBalancingAlgorithm.THRESHOLD_BASED
        )
        
    def test_engine_get_current_state(self):
        """Test getting simulation state snapshot."""
        self.engine.initialize()
        
        if hasattr(self.engine, 'get_current_state'):
            state = self.engine.get_current_state()
            self.assertIsNotNone(state)


class TestSimulationResult(unittest.TestCase):
    """Test SimulationResult data class."""
    
    def test_result_creation(self):
        """Test result object creation."""
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        result = engine.run()
        
        self.assertIsInstance(result, SimulationResult)
        self.assertIsNotNone(result.config)
        
    def test_result_has_metrics(self):
        """Test result contains metrics."""
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        result = engine.run()
        
        # Should have some form of metrics
        if hasattr(result, 'system_metrics'):
            self.assertIsNotNone(result.system_metrics)


# =============================================================================
# TEST METRICS MODULE
# =============================================================================

class TestProcessMetrics(unittest.TestCase):
    """Test ProcessMetrics class."""
    
    def test_turnaround_time_calculation(self):
        """Test turnaround time calculation."""
        metrics = ProcessMetrics(
            pid=1,
            arrival_time=0,
            burst_time=10,
            start_time=2,
            completion_time=15,
            waiting_time=2,
            processor_id=0,
            migration_count=0,
            priority="MEDIUM"
        )
        
        # Turnaround = Completion - Arrival = 15 - 0 = 15
        self.assertEqual(metrics.turnaround_time, 15)
        
    def test_response_time_calculation(self):
        """Test response time calculation."""
        metrics = ProcessMetrics(
            pid=1,
            arrival_time=5,
            burst_time=10,
            start_time=8,
            completion_time=18,
            waiting_time=3,
            processor_id=0,
            migration_count=0,
            priority="MEDIUM"
        )
        
        # Response = Start - Arrival = 8 - 5 = 3
        self.assertEqual(metrics.response_time, 3)
        
    def test_normalized_turnaround(self):
        """Test normalized turnaround calculation."""
        metrics = ProcessMetrics(
            pid=1,
            arrival_time=0,
            burst_time=10,
            start_time=0,
            completion_time=20,
            waiting_time=0,
            processor_id=0,
            migration_count=0,
            priority="MEDIUM"
        )
        
        # Normalized = Turnaround / Burst = 20 / 10 = 2.0
        self.assertEqual(metrics.normalized_turnaround, 2.0)
        
    def test_metrics_to_dict(self):
        """Test metrics serialization."""
        metrics = ProcessMetrics(
            pid=1,
            arrival_time=0,
            burst_time=10,
            start_time=0,
            completion_time=10,
            waiting_time=0,
            processor_id=0,
            migration_count=0,
            priority="MEDIUM"
        )
        
        data = metrics.to_dict()
        self.assertEqual(data['pid'], 1)
        self.assertEqual(data['burst_time'], 10)


class TestSystemMetrics(unittest.TestCase):
    """Test SystemMetrics calculations."""
    
    def test_system_metrics_structure(self):
        """Test system metrics data structure."""
        # SystemMetrics uses default values, create instance
        metrics = SystemMetrics()
        metrics.total_processes = 10
        metrics.completed_processes = 10
        metrics.avg_turnaround_time = 25.0
        metrics.avg_waiting_time = 15.0
        metrics.avg_response_time = 5.0
        metrics.avg_utilization = 0.85
        metrics.total_throughput = 0.4
        metrics.load_variance = 0.02
        metrics.total_migrations = 3
        metrics.total_simulation_time = 25
        metrics.algorithm_name = "Round Robin"
        
        self.assertEqual(metrics.total_processes, 10)
        self.assertEqual(metrics.completed_processes, 10)
        self.assertAlmostEqual(metrics.avg_utilization, 0.85)
        
    def test_system_metrics_to_dict(self):
        """Test system metrics serialization."""
        metrics = SystemMetrics()
        metrics.total_processes = 10
        metrics.completed_processes = 10
        
        data = metrics.to_dict()
        self.assertIn('total_processes', data)
        self.assertIn('completed_processes', data)


class TestMetricsCalculator(unittest.TestCase):
    """Test MetricsCalculator class."""
    
    def test_calculator_creation(self):
        """Test calculator initialization."""
        calculator = MetricsCalculator()
        self.assertIsNotNone(calculator)
        
    def test_calculate_from_processes(self):
        """Test metrics calculation from processes."""
        calculator = MetricsCalculator()
        
        processes = [
            Process(pid=1, arrival_time=0, burst_time=10),
            Process(pid=2, arrival_time=2, burst_time=5),
        ]
        
        # Simulate completion
        processes[0].start_time = 0
        processes[0].completion_time = 10
        processes[0].state = ProcessState.COMPLETED
        processes[0].remaining_time = 0
        
        processes[1].start_time = 2
        processes[1].completion_time = 12
        processes[1].state = ProcessState.COMPLETED
        processes[1].remaining_time = 0
        
        if hasattr(calculator, 'calculate_process_metrics'):
            for proc in processes:
                metrics = calculator.calculate_process_metrics(proc)
                self.assertIsNotNone(metrics)


class TestMetricsComparator(unittest.TestCase):
    """Test MetricsComparator class."""
    
    def test_comparator_creation(self):
        """Test comparator initialization."""
        comparator = MetricsComparator()
        self.assertIsNotNone(comparator)
        
    def test_compare_algorithms(self):
        """Test algorithm comparison."""
        comparator = MetricsComparator()
        
        # Create sample results using actual SystemMetrics class
        result1 = SystemMetrics()
        result1.total_processes = 10
        result1.completed_processes = 10
        result1.avg_turnaround_time = 20.0
        result1.avg_waiting_time = 10.0
        result1.avg_response_time = 5.0
        result1.avg_utilization = 0.8
        result1.total_throughput = 0.5
        result1.load_variance = 0.05
        result1.total_migrations = 0
        result1.total_simulation_time = 20
        result1.algorithm_name = "Round Robin"
        
        result2 = SystemMetrics()
        result2.total_processes = 10
        result2.completed_processes = 10
        result2.avg_turnaround_time = 18.0
        result2.avg_waiting_time = 8.0
        result2.avg_response_time = 4.0
        result2.avg_utilization = 0.85
        result2.total_throughput = 0.55
        result2.load_variance = 0.03
        result2.total_migrations = 2
        result2.total_simulation_time = 18
        result2.algorithm_name = "Least Loaded"
        
        if hasattr(comparator, 'compare'):
            comparison = comparator.compare([result1, result2])
            self.assertIsNotNone(comparison)


# =============================================================================
# TEST UTILS MODULE
# =============================================================================

class TestDataExporter(unittest.TestCase):
    """Test DataExporter class."""
    
    def test_exporter_creation(self):
        """Test exporter initialization."""
        exporter = DataExporter()
        self.assertIsNotNone(exporter)
        
    def test_export_to_dict(self):
        """Test exporting data to dictionary."""
        exporter = DataExporter()
        
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        result = engine.run()
        
        # Check result has to_dict method
        if hasattr(result, 'to_dict'):
            data = result.to_dict()
            self.assertIsInstance(data, dict)


# =============================================================================
# INTEGRATION TESTS
# =============================================================================

class TestIntegration(unittest.TestCase):
    """Integration tests for the complete system."""
    
    def test_full_simulation_round_robin(self):
        """Test complete simulation with Round Robin."""
        config = SimulationConfig(num_processors=4, num_processes=10)
        engine = SimulationEngine(config)
        engine.initialize(algorithm=LoadBalancingAlgorithm.ROUND_ROBIN)
        
        result = engine.run()
        
        self.assertIsNotNone(result)
        self.assertGreater(result.system_metrics.completed_processes, 0)
        
    def test_full_simulation_least_loaded(self):
        """Test complete simulation with Least Loaded."""
        config = SimulationConfig(num_processors=4, num_processes=10)
        engine = SimulationEngine(config)
        engine.initialize(algorithm=LoadBalancingAlgorithm.LEAST_LOADED)
        
        result = engine.run()
        
        self.assertIsNotNone(result)
        self.assertGreater(result.system_metrics.completed_processes, 0)
        
    def test_full_simulation_threshold_based(self):
        """Test complete simulation with Threshold Based."""
        config = SimulationConfig(num_processors=4, num_processes=10)
        engine = SimulationEngine(config)
        engine.initialize(algorithm=LoadBalancingAlgorithm.THRESHOLD_BASED)
        
        result = engine.run()
        
        self.assertIsNotNone(result)
        self.assertGreater(result.system_metrics.completed_processes, 0)
        
    def test_all_processes_complete(self):
        """Test that all processes eventually complete."""
        config = SimulationConfig(num_processors=4, num_processes=20)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        self.assertEqual(
            result.system_metrics.completed_processes,
            result.system_metrics.total_processes
        )
        
    def test_algorithm_comparison(self):
        """Test comparing all algorithms."""
        config = SimulationConfig(num_processors=4, num_processes=15)
        
        results = {}
        for algorithm in LoadBalancingAlgorithm:
            engine = SimulationEngine(config)
            engine.initialize(algorithm=algorithm)
            result = engine.run()
            results[algorithm.value] = result
            
        # All should complete
        for name, result in results.items():
            self.assertEqual(
                result.system_metrics.completed_processes,
                result.system_metrics.total_processes,
                f"{name} did not complete all processes"
            )


# =============================================================================
# EDGE CASE TESTS
# =============================================================================

class TestEdgeCases(unittest.TestCase):
    """Test edge cases and boundary conditions."""
    
    def test_single_processor(self):
        """Test simulation with single processor."""
        config = SimulationConfig(num_processors=1, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 5)
        
    def test_maximum_processors(self):
        """Test simulation with maximum processors."""
        config = SimulationConfig(num_processors=8, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 5)
        
    def test_single_process(self):
        """Test simulation with single process."""
        config = SimulationConfig(num_processors=4, num_processes=1)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 1)
        
    def test_many_processes(self):
        """Test simulation with many processes."""
        config = SimulationConfig(num_processors=4, num_processes=50)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 50)
        
    def test_short_burst_times(self):
        """Test with very short burst times."""
        config = SimulationConfig(
            num_processors=4, 
            num_processes=10,
            min_burst_time=1,
            max_burst_time=3
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 10)
        
    def test_long_burst_times(self):
        """Test with longer burst times."""
        config = SimulationConfig(
            num_processors=4,
            num_processes=5,
            min_burst_time=15,
            max_burst_time=25
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 5)
        
    def test_processes_more_than_processors(self):
        """Test when processes outnumber processors."""
        config = SimulationConfig(num_processors=2, num_processes=10)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 10)
        
    def test_processors_more_than_processes(self):
        """Test when processors outnumber processes."""
        config = SimulationConfig(num_processors=8, num_processes=3)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 3)


# =============================================================================
# SCENARIO TESTS
# =============================================================================

class TestScenarios(unittest.TestCase):
    """Test realistic usage scenarios."""
    
    def test_scenario_light_workload(self):
        """Test light workload scenario."""
        # Few short processes on many processors
        config = SimulationConfig(
            num_processors=8,
            num_processes=5,
            min_burst_time=2,
            max_burst_time=5
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        # Should have low utilization
        self.assertEqual(result.system_metrics.completed_processes, 5)
        
    def test_scenario_heavy_workload(self):
        """Test heavy workload scenario."""
        # Many long processes on few processors
        config = SimulationConfig(
            num_processors=2,
            num_processes=20,
            min_burst_time=10,
            max_burst_time=20
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        # Should complete all despite heavy load
        self.assertEqual(result.system_metrics.completed_processes, 20)
        
    def test_scenario_balanced_workload(self):
        """Test balanced workload scenario."""
        config = SimulationConfig(
            num_processors=4,
            num_processes=16,
            min_burst_time=5,
            max_burst_time=10
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        self.assertEqual(result.system_metrics.completed_processes, 16)
        
    def test_scenario_bursty_arrivals(self):
        """Test bursty arrival pattern."""
        config = SimulationConfig(
            num_processors=4,
            num_processes=15
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 15)
        
    def test_scenario_spread_arrivals(self):
        """Test spread out arrival pattern."""
        config = SimulationConfig(
            num_processors=4,
            num_processes=10
        )
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        self.assertEqual(result.system_metrics.completed_processes, 10)


# =============================================================================
# PERFORMANCE TESTS
# =============================================================================

class TestPerformance(unittest.TestCase):
    """Basic performance tests."""
    
    def test_simulation_completes_in_reasonable_time(self):
        """Test that simulation doesn't hang."""
        config = SimulationConfig(num_processors=4, num_processes=30)
        engine = SimulationEngine(config)
        engine.initialize()
        
        start_time = time.time()
        result = engine.run()
        elapsed = time.time() - start_time
        
        # Should complete within reasonable time (5 seconds)
        self.assertLess(elapsed, 5.0)
        self.assertEqual(result.system_metrics.completed_processes, 30)
        
    def test_large_simulation_stability(self):
        """Test stability with larger simulation."""
        config = SimulationConfig(num_processors=8, num_processes=100)
        engine = SimulationEngine(config)
        engine.initialize()
        
        result = engine.run()
        
        # Should complete without errors
        self.assertEqual(result.system_metrics.completed_processes, 100)


# =============================================================================
# ERROR HANDLING TESTS
# =============================================================================

class TestErrorHandling(unittest.TestCase):
    """Test error handling and validation."""
    
    def test_invalid_processor_count_handled(self):
        """Test handling of edge case processor counts."""
        # Test with minimum
        config = SimulationConfig(num_processors=1)
        engine = SimulationEngine(config)
        engine.initialize()
        # Should not raise
        
    def test_zero_processes_handled(self):
        """Test handling of zero processes."""
        config = SimulationConfig(num_processors=4, num_processes=0)
        engine = SimulationEngine(config)
        
        # Should handle gracefully - may not initialize properly with 0 processes
        try:
            result = engine.initialize()
            if result:
                result = engine.run()
                self.assertEqual(result.system_metrics.completed_processes, 0)
        except (ValueError, ZeroDivisionError, IndexError):
            pass  # Acceptable - some implementations may reject 0 processes
            
    def test_negative_values_handled(self):
        """Test that negative values don't crash system."""
        # The system should either reject or handle gracefully
        try:
            config = SimulationConfig(num_processors=4, num_processes=-1)
            # If it allows creation, should handle during run
        except (ValueError, AssertionError):
            pass  # Expected behavior
            
    def test_engine_double_initialize(self):
        """Test initializing already initialized engine."""
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        
        # Initializing again should be safe
        try:
            engine.initialize()
        except Exception:
            pass  # Some implementations may raise
            
    def test_engine_stop_not_started(self):
        """Test stopping engine that wasn't started."""
        config = SimulationConfig(num_processors=4, num_processes=5)
        engine = SimulationEngine(config)
        engine.initialize()
        
        # Should handle gracefully
        try:
            engine.stop()
        except Exception:
            pass  # Acceptable to raise


# =============================================================================
# TEST AI BALANCER
# =============================================================================

class TestQLearningBalancer(unittest.TestCase):
    """Test Q-Learning load balancer."""
    
    def setUp(self):
        """Set up test fixtures."""
        from ai_balancer import QLearningBalancer, QLearningAgent, StateEncoder
        self.config = SimulationConfig(num_processors=4)
        self.balancer = QLearningBalancer(config=self.config, num_processors=4)
        self.manager = ProcessorManager(num_processors=4)
        self.processors = list(self.manager)
        
    def test_balancer_creation(self):
        """Test Q-Learning balancer initialization."""
        self.assertIsNotNone(self.balancer)
        self.assertEqual(self.balancer.algorithm_type, LoadBalancingAlgorithm.Q_LEARNING)
        self.assertEqual(self.balancer.name, "AI (Q-Learning)")
        
    def test_process_assignment(self):
        """Test that balancer assigns processes."""
        process = Process(pid=1, burst_time=10)
        selected = self.balancer.assign_process(process, self.processors)
        
        self.assertIsNotNone(selected)
        self.assertIn(selected, self.processors)
        self.assertEqual(process.processor_id, selected.processor_id)
        
    def test_training_mode_toggle(self):
        """Test switching between training and exploitation modes."""
        self.assertTrue(self.balancer.agent.training_mode)
        
        self.balancer.set_training_mode(False)
        self.assertFalse(self.balancer.agent.training_mode)
        
        self.balancer.set_training_mode(True)
        self.assertTrue(self.balancer.agent.training_mode)
        
    def test_get_statistics(self):
        """Test statistics retrieval."""
        stats = self.balancer.get_statistics()
        
        self.assertIn('episode_count', stats)
        self.assertIn('epsilon', stats)
        self.assertIn('q_table_size', stats)
        self.assertIn('training_mode', stats)
        
    def test_multiple_assignments(self):
        """Test multiple process assignments."""
        generator = ProcessGenerator(config=self.config)
        processes = generator.generate_processes(10)
        
        for process in processes:
            selected = self.balancer.assign_process(process, self.processors)
            self.assertIsNotNone(selected)
            
        self.assertEqual(self.balancer.assignment_count, 10)
        
    def test_process_completed_feedback(self):
        """Test reward feedback mechanism."""
        process = Process(pid=1, burst_time=10)
        process.start_time = 0
        process.completion_time = 12
        
        selected = self.balancer.assign_process(process, self.processors)
        
        # Simulate process completion
        self.balancer.process_completed(process, self.processors)
        
        # Agent should have received update
        stats = self.balancer.get_statistics()
        self.assertGreater(stats['total_steps'], 0)
        

class TestQLearningAgent(unittest.TestCase):
    """Test Q-Learning agent."""
    
    def setUp(self):
        """Set up test fixtures."""
        from ai_balancer import QLearningAgent, QLearningConfig
        self.agent = QLearningAgent(num_processors=4)
        
    def test_agent_creation(self):
        """Test agent initialization."""
        self.assertEqual(self.agent.num_processors, 4)
        self.assertEqual(self.agent.epsilon, self.agent.config.epsilon_start)
        self.assertTrue(self.agent.training_mode)
        
    def test_action_selection(self):
        """Test action selection."""
        from ai_balancer import SystemState
        state = SystemState(
            load_levels=(0, 0, 0, 0),
            queue_levels=(0, 0, 0, 0),
            process_priority=2,
            process_burst_bucket=1
        )
        
        action = self.agent.get_action(state)
        self.assertIn(action, range(4))
        
    def test_q_value_update(self):
        """Test Q-value update."""
        from ai_balancer import SystemState
        state = SystemState(
            load_levels=(0, 1, 0, 0),
            queue_levels=(0, 1, 0, 0),
            process_priority=2,
            process_burst_bucket=1
        )
        
        # Get action and update
        action = self.agent.get_action(state)
        self.agent.update(reward=-1.0, next_state=state, done=False)
        
        # Q-table should have entry
        self.assertIn(state, self.agent.q_table)
        
    def test_epsilon_decay(self):
        """Test exploration rate decay."""
        from ai_balancer import SystemState
        initial_epsilon = self.agent.epsilon
        
        state = SystemState(
            load_levels=(0, 0, 0, 0),
            queue_levels=(0, 0, 0, 0),
            process_priority=2,
            process_burst_bucket=1
        )
        
        # Complete an episode
        for _ in range(10):
            self.agent.get_action(state)
            self.agent.update(reward=-1.0, next_state=state, done=False)
        
        # Mark episode complete
        self.agent.update(reward=-1.0, next_state=None, done=True)
        
        # Epsilon should have decayed
        self.assertLess(self.agent.epsilon, initial_epsilon)
        
    def test_reset(self):
        """Test agent reset."""
        from ai_balancer import SystemState
        state = SystemState(
            load_levels=(0, 0, 0, 0),
            queue_levels=(0, 0, 0, 0),
            process_priority=2,
            process_burst_bucket=1
        )
        
        # Do some learning
        self.agent.get_action(state)
        self.agent.update(reward=-1.0, next_state=state, done=True)
        
        # Reset
        self.agent.reset()
        
        self.assertEqual(len(self.agent.q_table), 0)
        self.assertEqual(self.agent.episode_count, 0)
        

class TestStateEncoder(unittest.TestCase):
    """Test state encoding for Q-Learning."""
    
    def setUp(self):
        """Set up test fixtures."""
        from ai_balancer import StateEncoder
        self.encoder = StateEncoder()
        self.manager = ProcessorManager(num_processors=4)
        self.processors = list(self.manager)
        
    def test_encode_empty_state(self):
        """Test encoding empty system state."""
        process = Process(pid=1, burst_time=5)
        state = self.encoder.encode(self.processors, process)
        
        self.assertIsNotNone(state)
        self.assertEqual(len(state.load_levels), 4)
        self.assertEqual(len(state.queue_levels), 4)
        
    def test_encode_loaded_state(self):
        """Test encoding with some load."""
        # Add processes to first processor
        for i in range(3):
            p = Process(pid=i, burst_time=10)
            self.processors[0].add_process(p)
            
        process = Process(pid=99, burst_time=5)
        state = self.encoder.encode(self.processors, process)
        
        # First processor should have higher load level
        self.assertGreater(state.load_levels[0], state.load_levels[1])
        
    def test_state_hashable(self):
        """Test that states are hashable for Q-table."""
        process = Process(pid=1, burst_time=5)
        state = self.encoder.encode(self.processors, process)
        
        # Should be usable as dict key
        test_dict = {state: 1.0}
        self.assertEqual(test_dict[state], 1.0)


class TestExperienceReplay(unittest.TestCase):
    """Test experience replay buffer."""
    
    def setUp(self):
        """Set up test fixtures."""
        from ai_balancer import ReplayBuffer, Experience, SystemState
        self.buffer = ReplayBuffer(capacity=100)
        
    def test_buffer_add(self):
        """Test adding experiences."""
        from ai_balancer import Experience, SystemState
        state = SystemState((0,), (0,), 1, 1)
        exp = Experience(state=state, action=0, reward=1.0, next_state=state, done=False)
        
        self.buffer.add(exp)
        self.assertEqual(len(self.buffer), 1)
        
    def test_buffer_sample(self):
        """Test sampling from buffer."""
        from ai_balancer import Experience, SystemState
        
        # Add 10 experiences
        for i in range(10):
            state = SystemState((i % 3,), (0,), 1, 1)
            exp = Experience(state=state, action=i % 4, reward=-1.0, next_state=state, done=False)
            self.buffer.add(exp)
            
        # Sample batch
        batch = self.buffer.sample(5)
        self.assertEqual(len(batch), 5)
        
    def test_buffer_capacity(self):
        """Test buffer respects capacity."""
        from ai_balancer import Experience, SystemState
        
        # Add more than capacity
        for i in range(150):
            state = SystemState((i % 3,), (0,), 1, 1)
            exp = Experience(state=state, action=0, reward=1.0, next_state=state, done=False)
            self.buffer.add(exp)
            
        self.assertEqual(len(self.buffer), 100)


# =============================================================================
# DQN TESTS
# =============================================================================

class TestDQNBalancer(unittest.TestCase):
    """Test Deep Q-Network load balancer."""
    
    @classmethod
    def setUpClass(cls):
        """Check if PyTorch is available."""
        try:
            from dqn_balancer import TORCH_AVAILABLE, DQNBalancer
            cls.torch_available = TORCH_AVAILABLE
        except ImportError:
            cls.torch_available = False
    
    def setUp(self):
        """Set up test fixtures."""
        if not self.torch_available:
            self.skipTest("PyTorch not available")
        
        from dqn_balancer import DQNBalancer, DQNConfig
        self.config = SimulationConfig(num_processors=4, num_processes=10)
        self.dqn_config = DQNConfig(
            min_replay_size=10,  # Lower for testing
            batch_size=4,
            epsilon_decay_steps=100
        )
        self.balancer = DQNBalancer(
            config=self.config, 
            dqn_config=self.dqn_config,
            num_processors=4
        )
        self.manager = ProcessorManager(num_processors=4)
        self.processors = list(self.manager)
        
    def test_initialization(self):
        """Test DQN balancer initialization."""
        from config import LoadBalancingAlgorithm
        self.assertEqual(self.balancer.algorithm_type, LoadBalancingAlgorithm.DQN)
        self.assertEqual(self.balancer.name, "AI (DQN)")
        self.assertEqual(self.balancer.num_processors, 4)
        
    def test_assign_process(self):
        """Test process assignment."""
        process = Process(pid=1, burst_time=5)
        selected = self.balancer.assign_process(process, self.processors)
        
        self.assertIsNotNone(selected)
        self.assertIn(selected, self.processors)
        self.assertEqual(self.balancer.assignment_count, 1)
        
    def test_multiple_assignments(self):
        """Test multiple process assignments."""
        for i in range(10):
            process = Process(pid=i, burst_time=random.randint(2, 10))
            selected = self.balancer.assign_process(process, self.processors)
            self.assertIsNotNone(selected)
            
        self.assertEqual(self.balancer.assignment_count, 10)
        
    def test_training_mode(self):
        """Test training/evaluation mode switching."""
        self.balancer.set_training_mode(True)
        self.assertTrue(self.balancer.agent.training_mode)
        
        self.balancer.set_training_mode(False)
        self.assertFalse(self.balancer.agent.training_mode)
        
    def test_get_statistics(self):
        """Test statistics retrieval."""
        stats = self.balancer.get_statistics()
        
        self.assertIn('epsilon', stats)
        self.assertIn('total_steps', stats)
        self.assertIn('training_mode', stats)
        self.assertIn('decisions_made', stats)
        
    def test_process_completed_callback(self):
        """Test reward feedback mechanism."""
        process = Process(pid=1, burst_time=5, arrival_time=0)
        self.balancer.assign_process(process, self.processors)
        
        # Simulate process completion
        process.start_time = 1
        process.completion_time = 6
        process.state = ProcessState.COMPLETED
        
        # Should not raise
        self.balancer.process_completed(process, self.processors)


class TestDQNAgent(unittest.TestCase):
    """Test DQN agent components."""
    
    @classmethod
    def setUpClass(cls):
        """Check if PyTorch is available."""
        try:
            from dqn_balancer import TORCH_AVAILABLE, DQNAgent
            cls.torch_available = TORCH_AVAILABLE
        except ImportError:
            cls.torch_available = False
    
    def setUp(self):
        """Set up test fixtures."""
        if not self.torch_available:
            self.skipTest("PyTorch not available")
            
        from dqn_balancer import DQNAgent, DQNConfig
        self.config = DQNConfig(
            min_replay_size=10,
            batch_size=4,
            epsilon_decay_steps=100
        )
        self.agent = DQNAgent(state_size=17, action_size=4, config=self.config)
        
    def test_initialization(self):
        """Test agent initialization."""
        self.assertEqual(self.agent.state_size, 17)
        self.assertEqual(self.agent.action_size, 4)
        self.assertEqual(self.agent.epsilon, self.config.epsilon_start)
        
    def test_action_selection(self):
        """Test action selection."""
        import numpy as np
        state = np.random.randn(17).astype(np.float32)
        action = self.agent.select_action(state)
        
        self.assertIsInstance(action, int)
        self.assertGreaterEqual(action, 0)
        self.assertLess(action, 4)
        
    def test_step(self):
        """Test learning step."""
        import numpy as np
        state = np.random.randn(17).astype(np.float32)
        next_state = np.random.randn(17).astype(np.float32)
        
        self.agent.select_action(state)
        self.agent.step(reward=-1.0, next_state=next_state, done=False)
        
        self.assertEqual(len(self.agent.replay_buffer), 1)
        
    def test_epsilon_decay(self):
        """Test epsilon decays during training."""
        import numpy as np
        initial_epsilon = self.agent.epsilon
        
        # Multiple actions to trigger decay
        for _ in range(10):
            state = np.random.randn(17).astype(np.float32)
            self.agent.select_action(state)
            
        self.assertLess(self.agent.epsilon, initial_epsilon)
        
    def test_evaluation_mode(self):
        """Test evaluation mode uses minimal exploration."""
        self.agent.set_training_mode(False)
        self.assertEqual(self.agent.epsilon, self.config.epsilon_end)
        self.assertFalse(self.agent.training_mode)


class TestDQNetwork(unittest.TestCase):
    """Test DQN neural network architecture."""
    
    @classmethod
    def setUpClass(cls):
        """Check if PyTorch is available."""
        try:
            from dqn_balancer import TORCH_AVAILABLE
            cls.torch_available = TORCH_AVAILABLE
        except ImportError:
            cls.torch_available = False
    
    def setUp(self):
        """Set up test fixtures."""
        if not self.torch_available:
            self.skipTest("PyTorch not available")
            
    def test_network_forward(self):
        """Test network forward pass."""
        import torch
        from dqn_balancer import DQNetwork, DQNConfig
        
        config = DQNConfig()
        network = DQNetwork(state_size=17, action_size=4, config=config)
        
        state = torch.randn(1, 17)
        output = network(state)
        
        self.assertEqual(output.shape, (1, 4))
        
    def test_dueling_architecture(self):
        """Test dueling network architecture."""
        import torch
        from dqn_balancer import DQNetwork, DQNConfig
        
        config = DQNConfig(use_dueling=True)
        network = DQNetwork(state_size=17, action_size=4, config=config)
        
        self.assertTrue(hasattr(network, 'value_output'))
        self.assertTrue(hasattr(network, 'advantage_output'))
        
        state = torch.randn(1, 17)
        output = network(state)
        self.assertEqual(output.shape, (1, 4))
        
    def test_batch_forward(self):
        """Test batch processing."""
        import torch
        from dqn_balancer import DQNetwork, DQNConfig
        
        config = DQNConfig()
        network = DQNetwork(state_size=17, action_size=4, config=config)
        
        batch = torch.randn(32, 17)
        output = network(batch)
        
        self.assertEqual(output.shape, (32, 4))


class TestPrioritizedReplay(unittest.TestCase):
    """Test prioritized experience replay."""
    
    @classmethod
    def setUpClass(cls):
        """Check if PyTorch is available."""
        try:
            from dqn_balancer import TORCH_AVAILABLE
            cls.torch_available = TORCH_AVAILABLE
        except ImportError:
            cls.torch_available = False
    
    def setUp(self):
        """Set up test fixtures."""
        if not self.torch_available:
            self.skipTest("PyTorch not available")
            
        from dqn_balancer import PrioritizedReplayBuffer, DQNConfig, Transition
        import numpy as np
        
        self.config = DQNConfig()
        self.buffer = PrioritizedReplayBuffer(capacity=100, config=self.config)
        
    def test_add_transition(self):
        """Test adding transitions."""
        import numpy as np
        from dqn_balancer import Transition
        
        transition = Transition(
            state=np.zeros(17, dtype=np.float32),
            action=0,
            reward=1.0,
            next_state=np.zeros(17, dtype=np.float32),
            done=False
        )
        
        self.buffer.add(transition)
        self.assertEqual(len(self.buffer), 1)
        
    def test_sample_with_weights(self):
        """Test sampling returns importance weights."""
        import numpy as np
        from dqn_balancer import Transition
        
        # Add multiple transitions
        for i in range(20):
            transition = Transition(
                state=np.random.randn(17).astype(np.float32),
                action=i % 4,
                reward=float(i),
                next_state=np.random.randn(17).astype(np.float32),
                done=False
            )
            self.buffer.add(transition)
            
        transitions, indices, weights = self.buffer.sample(8)
        
        self.assertEqual(len(transitions), 8)
        self.assertIsNotNone(indices)
        self.assertIsNotNone(weights)
        self.assertEqual(len(weights), 8)


# =============================================================================
# TEST RUNNER
# =============================================================================

def run_all_tests(verbosity=2):
    """Run all tests with specified verbosity."""
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add all test classes
    test_classes = [
        TestConfig,
        TestProcess,
        TestProcessGenerator,
        TestProcessor,
        TestProcessorManager,
        TestRoundRobinBalancer,
        TestLeastLoadedBalancer,
        TestThresholdBasedBalancer,
        TestLoadBalancerFactory,
        TestQLearningBalancer,
        TestQLearningAgent,
        TestStateEncoder,
        TestExperienceReplay,
        TestDQNBalancer,
        TestDQNAgent,
        TestDQNetwork,
        TestPrioritizedReplay,
        TestSimulationEngine,
        TestSimulationResult,
        TestProcessMetrics,
        TestSystemMetrics,
        TestMetricsCalculator,
        TestMetricsComparator,
        TestDataExporter,
        TestIntegration,
        TestEdgeCases,
        TestScenarios,
        TestPerformance,
        TestErrorHandling,
    ]
    
    for test_class in test_classes:
        tests = loader.loadTestsFromTestCase(test_class)
        suite.addTests(tests)
        
    runner = unittest.TextTestRunner(verbosity=verbosity)
    result = runner.run(suite)
    
    return result


def run_quick_tests():
    """Run a quick subset of critical tests."""
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Only critical tests
    critical_tests = [
        TestConfig,
        TestProcess,
        TestProcessor,
        TestSimulationEngine,
        TestIntegration,
    ]
    
    for test_class in critical_tests:
        tests = loader.loadTestsFromTestCase(test_class)
        suite.addTests(tests)
        
    runner = unittest.TextTestRunner(verbosity=2)
    return runner.run(suite)


if __name__ == '__main__':
    print("=" * 70)
    print("DYNAMIC LOAD BALANCING SIMULATOR - COMPREHENSIVE TEST SUITE")
    print("=" * 70)
    print()
    
    # Enable logging for debugging if needed
    # logging.disable(logging.NOTSET)
    
    # Run all tests
    result = run_all_tests(verbosity=2)
    
    # Summary
    print()
    print("=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)
    print(f"Tests Run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Skipped: {len(result.skipped)}")
    print()
    
    if result.wasSuccessful():
        print(" ALL TESTS PASSED!")
    else:
        print(" SOME TESTS FAILED")
        
        if result.failures:
            print("\nFailures:")
            for test, trace in result.failures:
                print(f"  - {test}")
                
        if result.errors:
            print("\nErrors:")
            for test, trace in result.errors:
                print(f"  - {test}")
    
    print("=" * 70)
    
    # Exit with appropriate code
    sys.exit(0 if result.wasSuccessful() else 1)
